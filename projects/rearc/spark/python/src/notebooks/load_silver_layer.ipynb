{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b26c433",
      "metadata": {},
      "source": [
        "# Silver Layer: Data cleaning\n",
        "\n",
        "Read from bronze tables, apply cleaning (trim, cast, dedup, null handling), write to `development.silver`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27374ea3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "catalog = \"development\"\n",
        "schema_silver = \"silver\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70e2e14",
      "metadata": {},
      "source": [
        "# 1) Population: clean and write to development.silver.datausa_population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c7d2a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_population  = spark.table(f\"{catalog}.bronze.datausa_population_raw\")\n",
        "# Clean: trim strings, cast types, drop nulls in key columns, dedup\n",
        "df_pop_clean = (\n",
        "    df_population\n",
        "    .select(\n",
        "        F.trim(F.col(\"Nation ID\")).alias(\"nation_id\"),\n",
        "        F.trim(F.col(\"Nation\")).alias(\"nation\"),\n",
        "        F.col(\"Year\").cast(\"int\").alias(\"year\"),\n",
        "        F.col(\"Population\").cast(\"double\").alias(\"population\"),\n",
        "    )\n",
        "    .dropDuplicates([\"nation_id\", \"year\"])\n",
        "    .filter(F.col(\"year\").isNotNull() & F.col(\"population\").isNotNull())\n",
        ")\n",
        "df_pop_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema_silver}.datausa_population\")\n",
        "print(f\"Created {catalog}.{schema_silver}.datausa_population\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1b430d",
      "metadata": {},
      "source": [
        "# 2) BLS time series: clean and write to development.silver.bls_pr_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b64592",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_bls = spark.table(f\"{catalog}.bronze.bls_pr_data_raw\")\n",
        "# Clean: trim strings, cast types, dedup (same as bronze cleaning logic, applied here for silver)\n",
        "for c in df_bls.columns:\n",
        "    df_bls = df_bls.withColumnRenamed(c, c.strip())\n",
        "df_bls_clean = (\n",
        "    df_bls\n",
        "    .select(\n",
        "        F.trim(F.col(\"series_id\")).alias(\"series_id\"),\n",
        "        F.col(\"year\").cast(\"int\"),\n",
        "        F.trim(F.col(\"period\")).alias(\"period\"),\n",
        "        F.col(\"value\").cast(\"double\"),\n",
        "        F.trim(F.col(\"footnote_codes\")).alias(\"footnote_codes\"),\n",
        "    )\n",
        "    .dropDuplicates([\"series_id\", \"year\", \"period\"])\n",
        "    .filter(F.col(\"series_id\").isNotNull() & F.col(\"year\").isNotNull() & F.col(\"value\").isNotNull())\n",
        ")\n",
        "df_bls_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema_silver}.bls_pr_data\")\n",
        "print(f\"Created {catalog}.{schema_silver}.bls_pr_data\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
