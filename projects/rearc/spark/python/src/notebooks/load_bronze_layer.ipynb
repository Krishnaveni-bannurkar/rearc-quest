{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffe19223",
      "metadata": {},
      "source": [
        "# S3 to Bronze (Managed Delta)\n",
        "\n",
        "Read directly from S3 and write to managed Delta tables in `development.bronze`.  \n",
        "S3 = source of truth; Databricks bronze = Delta tables only (no volume copy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85bb7c0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.dbutils import DBUtils\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "dbutils = DBUtils(spark)\n",
        "\n",
        "bucket = \"kbannu-test1\"\n",
        "catalog = \"development\"\n",
        "schema = \"bronze\"\n",
        "\n",
        "\n",
        "sc = spark.sparkContext\n",
        "conf = sc._jsc.hadoopConfiguration() #I am using access keys, I can also create a IAM role and assign it to the cluster\n",
        "conf.set(\"fs.s3a.access.key\", dbutils.secrets.get(\"quest-aws\", \"access-key-id\"))\n",
        "conf.set(\"fs.s3a.secret.key\", dbutils.secrets.get(\"quest-aws\", \"secret-access-key\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6b606b",
      "metadata": {},
      "source": [
        "# 1) JSON from S3 → development.bronze.datausa_population_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf7e22b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "s3_key_json = \"datausa/acs_yg_total_population_1.json\"\n",
        "s3_path_json = f\"s3a://{bucket}/{s3_key_json}\"\n",
        "raw_json = spark.read.option(\"multiLine\", True).json(s3_path_json)\n",
        "df_pop = raw_json.select(F.explode(\"data\").alias(\"row\")).select(\"row.*\")\n",
        "df_pop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.datausa_population_raw\")\n",
        "print(f\"Created {catalog}.{schema}.datausa_population_raw from {s3_path_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068d7fae",
      "metadata": {},
      "source": [
        "# 2) CSV from S3 → development.bronze.bls_pr_data_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45b0bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "s3_key_csv = \"bls/pr/pub/time.series/pr/pr.data.0.Current\"\n",
        "s3_path_csv = f\"s3a://{bucket}/{s3_key_csv}\"\n",
        "df_csv_raw = (\n",
        "    spark.read.option(\"header\", True).option(\"delimiter\", \"\\t\").option(\"inferSchema\", True).csv(s3_path_csv)\n",
        ")\n",
        "for c in df_csv_raw.columns:\n",
        "    df_csv_raw = df_csv_raw.withColumnRenamed(c, c.strip())\n",
        "df_csv_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.bls_pr_data_raw\")\n",
        "print(f\"Created {catalog}.{schema}.bls_pr_data_raw from {s3_path_csv}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
